"""
ê³ ê¸‰ ë²•ë¥  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸
í˜•ì‚¬ë²• LLM ë°ì´í„°ë¥¼ í™œìš©í•œ ì •í™•ë„ ë†’ì€ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì‹œìŠ¤í…œ
"""

import os
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from utils.legal_data_processor import LegalDataProcessor
import pickle
import logging
from typing import List, Dict, Optional, Tuple
from tqdm import tqdm

class AdvancedLegalVectorDB:
    """ê³ ê¸‰ ë²•ë¥  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 model_name: str = "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
                 index_path: str = "legal_vector.index",
                 metadata_path: str = "legal_metadata.pkl"):
        
        self.model_name = model_name
        self.index_path = index_path
        self.metadata_path = metadata_path
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        try:
            self.model = SentenceTransformer(model_name)
            self.logger.info(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
        except Exception as e:
            self.logger.warning(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            try:
                self.model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
                self.logger.info("ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©: all-MiniLM-L6-v2")
            except Exception as e2:
                self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì™„ì „ ì‹¤íŒ¨: {e2}")
                self.model = None
        
        self.index = None
        self.metadata = []
        self.dimension = 384  # ê¸°ë³¸ ì„ë² ë”© ì°¨ì›
    
    def build_knowledge_base(self) -> Dict:
        """í˜•ì‚¬ë²• LLM ë°ì´í„°ë¡œë¶€í„° ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶•"""
        self.logger.info("í˜•ì‚¬ë²• LLM ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
        
        processor = LegalDataProcessor()
        
        try:
            # ì§€ì‹ë² ì´ìŠ¤ ìƒì„±
            knowledge_base = processor.create_knowledge_base()
            
            # ì„ë² ë”©ìš© ë°ì´í„° ì¤€ë¹„
            embedding_data = processor.get_embedding_data()
            
            self.logger.info(f"ì´ {len(embedding_data)}ê°œì˜ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ")
            
            # ì§€ì‹ë² ì´ìŠ¤ ì €ì¥
            processor.save_knowledge_base(knowledge_base, "enhanced_legal_knowledge_base.json")
            
            return knowledge_base, embedding_data
            
        except Exception as e:
            self.logger.error(f"ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• ì˜¤ë¥˜: {e}")
            return {}, []
    
    def create_embeddings(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        if self.model is None:
            self.logger.error("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return np.array([])
        
        embeddings = []
        
        try:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„±
            for i in tqdm(range(0, len(texts), batch_size), desc="ì„ë² ë”© ìƒì„±"):
                batch_texts = texts[i:i + batch_size]
                batch_embeddings = self.model.encode(
                    batch_texts,
                    convert_to_tensor=False,
                    show_progress_bar=False
                )
                embeddings.extend(batch_embeddings)
            
            embeddings_array = np.array(embeddings).astype('float32')
            self.dimension = embeddings_array.shape[1]
            
            self.logger.info(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings_array.shape}")
            return embeddings_array
            
        except Exception as e:
            self.logger.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return np.array([])
    
    def build_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            # IVF + PQ ì¸ë±ìŠ¤ (ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— íš¨ìœ¨ì )
            if len(embeddings) > 1000:
                nlist = min(int(np.sqrt(len(embeddings))), 100)  # í´ëŸ¬ìŠ¤í„° ìˆ˜
                quantizer = faiss.IndexFlatIP(self.dimension)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
                index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
                
                # í›ˆë ¨
                index.train(embeddings)
                index.add(embeddings)
                index.nprobe = min(10, nlist)  # ê²€ìƒ‰ ì‹œ íƒìƒ‰í•  í´ëŸ¬ìŠ¤í„° ìˆ˜
                
                self.logger.info(f"IVF ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(embeddings)}ê°œ ë²¡í„°, {nlist}ê°œ í´ëŸ¬ìŠ¤í„°")
                
            else:
                # ì‘ì€ ë°ì´í„°ì…‹ì˜ ê²½ìš° Flat ì¸ë±ìŠ¤
                index = faiss.IndexFlatIP(self.dimension)
                index.add(embeddings)
                
                self.logger.info(f"Flat ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(embeddings)}ê°œ ë²¡í„°")
            
            return index
            
        except Exception as e:
            self.logger.error(f"FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì˜¤ë¥˜: {e}")
            return None
    
    def build_vector_db(self):
        """ì „ì²´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• í”„ë¡œì„¸ìŠ¤"""
        self.logger.info("ğŸš€ ê³ ê¸‰ ë²•ë¥  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘")
        
        # 1. ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶•
        knowledge_base, embedding_data = self.build_knowledge_base()
        
        if not embedding_data:
            self.logger.error("ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [item['text'] for item in embedding_data]
        
        # 3. ì„ë² ë”© ìƒì„±
        self.logger.info("ë²¡í„° ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = self.create_embeddings(texts)
        
        if embeddings.size == 0:
            self.logger.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return False
        
        # 4. FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
        self.logger.info("FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        self.index = self.build_faiss_index(embeddings)
        
        if self.index is None:
            self.logger.error("ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
            return False
        
        # 5. ë©”íƒ€ë°ì´í„° ì €ì¥
        self.metadata = embedding_data
        
        # 6. íŒŒì¼ ì €ì¥
        self.save_index()
        self.save_metadata()
        
        self.logger.info("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        return True
    
    def save_index(self):
        """FAISS ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            faiss.write_index(self.index, self.index_path)
            self.logger.info(f"ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.index_path}")
        except Exception as e:
            self.logger.error(f"ì¸ë±ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def save_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            with open(self.metadata_path, 'wb') as f:
                pickle.dump(self.metadata, f)
            self.logger.info(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.metadata_path}")
        except Exception as e:
            self.logger.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def load_index(self) -> bool:
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            if os.path.exists(self.index_path):
                self.index = faiss.read_index(self.index_path)
                self.logger.info(f"ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.index_path}")
                return True
            return False
        except Exception as e:
            self.logger.error(f"ì¸ë±ìŠ¤ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def load_metadata(self) -> bool:
        """ì €ì¥ëœ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        try:
            if os.path.exists(self.metadata_path):
                with open(self.metadata_path, 'rb') as f:
                    self.metadata = pickle.load(f)
                self.logger.info(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.metadata_path}")
                return True
            return False
        except Exception as e:
            self.logger.error(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def search_similar_cases(self, 
                           query: str, 
                           top_k: int = 5, 
                           case_type: Optional[str] = None) -> List[Dict]:
        """ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰"""
        if self.model is None or self.index is None:
            self.logger.error("ëª¨ë¸ ë˜ëŠ” ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ")
            return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.model.encode([query], convert_to_tensor=False)
            query_embedding = np.array(query_embedding).astype('float32')
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰
            scores, indices = self.index.search(query_embedding, top_k * 2)  # ì—¬ìœ ë¶„ í™•ë³´
            
            results = []
            added_cases = set()  # ì¤‘ë³µ ì œê±°ìš©
            
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx >= len(self.metadata):
                    continue
                
                case_data = self.metadata[idx].copy()
                case_data['similarity_score'] = float(score)
                case_data['rank'] = i + 1
                
                # ì‚¬ê±´ ìœ í˜• í•„í„°ë§
                if case_type and case_data.get('metadata', {}).get('type') != case_type:
                    continue
                
                # ì¤‘ë³µ ì œê±° (ê°™ì€ ì‚¬ê±´ ID)
                case_id = case_data.get('metadata', {}).get('case_id', '')
                if case_id and case_id in added_cases:
                    continue
                
                results.append(case_data)
                if case_id:
                    added_cases.add(case_id)
                
                if len(results) >= top_k:
                    break
            
            self.logger.info(f"ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê±´ ë°˜í™˜")
            return results
            
        except Exception as e:
            self.logger.error(f"ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_case_statistics(self) -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´"""
        if not self.metadata:
            return {}
        
        stats = {
            'total_cases': len(self.metadata),
            'case_types': {},
            'avg_text_length': 0
        }
        
        text_lengths = []
        
        for item in self.metadata:
            # ì‚¬ê±´ ìœ í˜•ë³„ í†µê³„
            case_type = item.get('metadata', {}).get('type', 'Unknown')
            stats['case_types'][case_type] = stats['case_types'].get(case_type, 0) + 1
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„
            text_length = len(item.get('text', ''))
            text_lengths.append(text_length)
        
        if text_lengths:
            stats['avg_text_length'] = sum(text_lengths) / len(text_lengths)
            stats['min_text_length'] = min(text_lengths)
            stats['max_text_length'] = max(text_lengths)
        
        return stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ›ï¸  ê³ ê¸‰ í˜•ì‚¬ë²• ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # ë²¡í„° DB ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    vector_db = AdvancedLegalVectorDB()
    
    # ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
    if vector_db.load_index() and vector_db.load_metadata():
        print("âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë°œê²¬ë¨")
        
        rebuild = input("ìƒˆë¡œ êµ¬ì¶•í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if rebuild != 'y':
            print("ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            # í†µê³„ ì •ë³´ ì¶œë ¥
            stats = vector_db.get_case_statistics()
            print(f"\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
            print(f"ì „ì²´ ì‚¬ë¡€: {stats.get('total_cases', 0)}ê±´")
            print(f"ì‚¬ê±´ ìœ í˜•ë³„:")
            for case_type, count in stats.get('case_types', {}).items():
                print(f"  - {case_type}: {count}ê±´")
            
            # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            test_query = input("\nê²€ìƒ‰í•  ì§ˆì˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì—”í„° ì‹œ ê¸°ë³¸ ì˜ˆì‹œ): ").strip()
            if not test_query:
                test_query = "êµí†µì‚¬ê³ ë¡œ ì¸í•œ ì†í•´ë°°ìƒì±…ì„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
            
            print(f"\nğŸ” ê²€ìƒ‰ ì§ˆì˜: {test_query}")
            results = vector_db.search_similar_cases(test_query, top_k=3)
            
            for i, result in enumerate(results, 1):
                print(f"\n{i}. ìœ ì‚¬ë„: {result['similarity_score']:.4f}")
                print(f"   ìœ í˜•: {result.get('metadata', {}).get('type', 'Unknown')}")
                print(f"   ë‚´ìš©: {result.get('text', '')[:200]}...")
            
            return
    
    # ìƒˆë¡œ êµ¬ì¶•
    print("ğŸš€ ìƒˆë¡œìš´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    success = vector_db.build_vector_db()
    
    if success:
        print("\nğŸ‰ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì„±ê³µ!")
        
        # í†µê³„ ì •ë³´ ì¶œë ¥
        stats = vector_db.get_case_statistics()
        print(f"\nğŸ“Š êµ¬ì¶•ëœ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
        print(f"ì „ì²´ ì‚¬ë¡€: {stats.get('total_cases', 0)}ê±´")
        print(f"í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {stats.get('avg_text_length', 0):.0f}ì")
        print(f"ì‚¬ê±´ ìœ í˜•ë³„:")
        for case_type, count in stats.get('case_types', {}).items():
            print(f"  - {case_type}: {count}ê±´")
        
    else:
        print("âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")


if __name__ == "__main__":
    main() 