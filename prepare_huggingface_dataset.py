#!/usr/bin/env python3
"""
í˜•ì‚¬ë²• LLM ë°ì´í„°ë¥¼ í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import json
import os
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from datasets import Dataset, DatasetDict
from huggingface_hub import HfApi, create_repo

def load_json_files(data_dir):
    """JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  í†µí•©ëœ ë°ì´í„°ì…‹ ìƒì„±"""
    all_data = []
    data_types = ['ê²°ì •ë¡€_QA', 'ê²°ì •ë¡€_SUM', 'ë²•ë ¹_QA', 'íŒê²°ë¬¸_QA', 'íŒê²°ë¬¸_SUM', 'í•´ì„ë¡€_QA', 'í•´ì„ë¡€_SUM']
    
    print(f"ğŸ“‚ ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    
    for data_type in data_types:
        # Training ë°ì´í„°
        train_dir = Path(data_dir) / "Training" / "02.ë¼ë²¨ë§ë°ì´í„°" / f"TL_{data_type}"
        if train_dir.exists():
            print(f"ğŸ“„ {data_type} Training ë°ì´í„° ë¡œë”©...")
            load_from_directory(train_dir, all_data, "train", data_type)
        
        # Validation ë°ì´í„° (ì••ì¶•íŒŒì¼ì´ë¯€ë¡œ ìŠ¤í‚µ)
        # val_dir = Path(data_dir) / "Validation" / "02.ë¼ë²¨ë§ë°ì´í„°" / f"VL_{data_type}"
    
    print(f"âœ… ì´ {len(all_data)}ê°œì˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    return all_data

def load_from_directory(directory, all_data, split, data_type):
    """íŠ¹ì • ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ë“¤ì„ ë¡œë“œ"""
    json_files = list(directory.glob("*.json"))
    print(f"   - {len(json_files)}ê°œ íŒŒì¼ ë°œê²¬")
    
    for json_file in tqdm(json_files, desc=f"Loading {data_type}"):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # ë°ì´í„° êµ¬ì¡° ë³€í™˜
                processed_data = {
                    'id': data.get('info', {}).get('determintId', ''),
                    'case_number': data.get('info', {}).get('caseNum', ''),
                    'case_name': data.get('info', {}).get('caseName', ''),
                    'court_code': data.get('info', {}).get('courtCode', ''),
                    'final_date': data.get('info', {}).get('finalDate', ''),
                    'law_class': data.get('info', {}).get('lawClass', ''),
                    'docu_type': data.get('info', {}).get('DocuType', ''),
                    'case_code': data.get('info', {}).get('caseCode', ''),
                    'instruction': data.get('label', {}).get('instruction', ''),
                    'input': data.get('label', {}).get('input', ''),
                    'output': data.get('label', {}).get('output', ''),
                    'origin_word_count': data.get('label', {}).get('originwordCnt', ''),
                    'label_word_count': data.get('label', {}).get('labelwordCnt', ''),
                    'data_type': data_type,
                    'split': split,
                    'file_name': json_file.name
                }
                
                all_data.append(processed_data)
                
        except Exception as e:
            print(f"âŒ {json_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

def create_huggingface_dataset(all_data):
    """í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    print("ğŸ”„ ë°ì´í„°ì…‹ ë³€í™˜ ì¤‘...")
    
    # ë°ì´í„°íƒ€ì…ë³„ë¡œ ë¶„ë¥˜
    datasets = {}
    
    for data_type in ['ê²°ì •ë¡€_QA', 'ê²°ì •ë¡€_SUM', 'ë²•ë ¹_QA', 'íŒê²°ë¬¸_QA', 'íŒê²°ë¬¸_SUM', 'í•´ì„ë¡€_QA', 'í•´ì„ë¡€_SUM']:
        filtered_data = [item for item in all_data if item['data_type'] == data_type]
        if filtered_data:
            datasets[data_type] = Dataset.from_list(filtered_data)
            print(f"âœ… {data_type}: {len(filtered_data)}ê°œ ë°ì´í„°")
    
    # ì „ì²´ í†µí•© ë°ì´í„°ì…‹
    datasets['all'] = Dataset.from_list(all_data)
    print(f"âœ… ì „ì²´: {len(all_data)}ê°œ ë°ì´í„°")
    
    return DatasetDict(datasets)

def upload_to_huggingface(dataset_dict, repo_name, token=None):
    """í—ˆê¹…í˜ì´ìŠ¤ì— ë°ì´í„°ì…‹ ì—…ë¡œë“œ"""
    print(f"ğŸš€ í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë¡œë“œ: {repo_name}")
    
    if not token:
        print("âŒ í—ˆê¹…í˜ì´ìŠ¤ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("https://huggingface.co/settings/tokens ì—ì„œ í† í°ì„ ìƒì„±í•˜ì„¸ìš”.")
        return False
    
    try:
        # ë ˆí¬ì§€í† ë¦¬ ìƒì„±
        api = HfApi(token=token)
        create_repo(
            repo_id=repo_name,
            token=token,
            repo_type="dataset",
            exist_ok=True
        )
        
        # ë°ì´í„°ì…‹ ì—…ë¡œë“œ
        dataset_dict.push_to_hub(
            repo_id=repo_name,
            token=token
        )
        
        print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: https://huggingface.co/datasets/{repo_name}")
        return True
        
    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ í˜•ì‚¬ë²• LLM ë°ì´í„°ì…‹ í—ˆê¹…í˜ì´ìŠ¤ ì¤€ë¹„ ì‹œì‘...")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    data_dir = "04.í˜•ì‚¬ë²• LLM ì‚¬ì „í•™ìŠµ ë° Instruction Tuning ë°ì´í„°/3.ê°œë°©ë°ì´í„°/1.ë°ì´í„°"
    
    if not os.path.exists(data_dir):
        print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return
    
    # 1. ë°ì´í„° ë¡œë“œ
    all_data = load_json_files(data_dir)
    
    if not all_data:
        print("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜
    dataset_dict = create_huggingface_dataset(all_data)
    
    # 3. ë¡œì»¬ ì €ì¥ (ì„ íƒì‚¬í•­)
    print("ğŸ’¾ ë¡œì»¬ì— ë°ì´í„°ì…‹ ì €ì¥...")
    dataset_dict.save_to_disk("korean_legal_dataset")
    
    # 4. ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
    print("\nğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:")
    for key, dataset in dataset_dict.items():
        print(f"  - {key}: {len(dataset)}ê°œ ìƒ˜í”Œ")
        if len(dataset) > 0:
            print(f"    ì»¬ëŸ¼: {list(dataset.column_names)}")
    
    # 5. ìƒ˜í”Œ ë°ì´í„° í™•ì¸
    if len(dataset_dict['all']) > 0:
        print(f"\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„°:")
        sample = dataset_dict['all'][0]
        for key, value in sample.items():
            if isinstance(value, str) and len(value) > 100:
                print(f"  {key}: {value[:100]}...")
            else:
                print(f"  {key}: {value}")
    
    print("\nğŸ‰ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!")
    print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. í—ˆê¹…í˜ì´ìŠ¤ ê³„ì • ìƒì„±: https://huggingface.co/join")
    print("2. í† í° ìƒì„±: https://huggingface.co/settings/tokens")
    print("3. ì—…ë¡œë“œ ì‹¤í–‰:")
    print("   python prepare_huggingface_dataset.py --upload --token YOUR_TOKEN --repo YOUR_USERNAME/korean-legal-dataset")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="í˜•ì‚¬ë²• LLM ë°ì´í„°ì…‹ í—ˆê¹…í˜ì´ìŠ¤ ì¤€ë¹„")
    parser.add_argument("--upload", action="store_true", help="í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë¡œë“œ")
    parser.add_argument("--token", type=str, help="í—ˆê¹…í˜ì´ìŠ¤ í† í°")
    parser.add_argument("--repo", type=str, default="korean-legal-dataset", help="ë ˆí¬ì§€í† ë¦¬ ì´ë¦„")
    
    args = parser.parse_args()
    
    if args.upload:
        if not args.token:
            print("âŒ --token ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            # ì—…ë¡œë“œ ëª¨ë“œ
            main()
            
            # ë°ì´í„°ì…‹ ë¡œë“œ í›„ ì—…ë¡œë“œ
            from datasets import load_from_disk
            dataset_dict = load_from_disk("korean_legal_dataset")
            upload_to_huggingface(dataset_dict, args.repo, args.token)
    else:
        # ì¤€ë¹„ë§Œ ì‹¤í–‰
        main() 