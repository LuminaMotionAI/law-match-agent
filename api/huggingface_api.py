"""
í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” API í´ë˜ìŠ¤
"""

import os
from typing import List, Dict, Optional
from datasets import load_dataset
import pandas as pd
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class HuggingFaceAPI:
    """í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” ë²•ë¥  ê²€ìƒ‰ API"""
    
    def __init__(self, dataset_name="LuminaMotionAI/korean-legal-dataset"):
        """
        Args:
            dataset_name: í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ ì´ë¦„
        """
        self.dataset_name = dataset_name
        self.dataset = None
        self.encoder = None
        self.embeddings_cache = {}
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_encoder()
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        self._load_dataset()
    
    def _initialize_encoder(self):
        """í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.encoder = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI')
            print("âœ… í•œêµ­ì–´ SBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.encoder = None
    
    def _load_dataset(self):
        """í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ"""
        try:
            print(f"ğŸ“¥ í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ë°ì´í„°ì…‹ ë¡œë”©: {self.dataset_name}")
            self.dataset = load_dataset(self.dataset_name)
            print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.dataset['all'])}ê°œ ë°ì´í„°")
            
            # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ (ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
            self.df = self.dataset['all'].to_pandas()
            print(f"âœ… ë°ì´í„°í”„ë ˆì„ ë³€í™˜ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ë¡œì»¬ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
            self._load_local_dataset()
    
    def _load_local_dataset(self):
        """ë¡œì»¬ ë°ì´í„°ì…‹ ë¡œë“œ (ë°±ì—…)"""
        try:
            from datasets import load_from_disk
            self.dataset = load_from_disk("korean_legal_dataset")
            self.df = self.dataset['all'].to_pandas()
            print(f"âœ… ë¡œì»¬ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ ë°ì´í„°")
        except Exception as e:
            print(f"âŒ ë¡œì»¬ ë°ì´í„°ì…‹ ë¡œë“œë„ ì‹¤íŒ¨: {e}")
            self.dataset = None
            self.df = pd.DataFrame()
    
    def search_similar_cases(self, query: str, top_k: int = 5, case_type: str = None) -> List[Dict]:
        """ìœ ì‚¬í•œ ì‚¬ë¡€ ê²€ìƒ‰"""
        if self.df.empty or self.encoder is None:
            return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.encoder.encode([query])
            
            # ë°ì´í„° í•„í„°ë§
            search_df = self.df.copy()
            if case_type and case_type != "ì „ì²´":
                case_type_map = {
                    "í•´ì„ë¡€": "í•´ì„ë¡€_QA",
                    "íŒê²°ë¬¸": "íŒê²°ë¬¸_QA", 
                    "ê²°ì •ë¡€": "ê²°ì •ë¡€_QA",
                    "ë²•ë ¹": "ë²•ë ¹_QA"
                }
                if case_type in case_type_map:
                    search_df = search_df[search_df['data_type'].str.contains(case_type_map[case_type])]
            
            if search_df.empty:
                return []
            
            # í…ìŠ¤íŠ¸ ì„ë² ë”© (ìºì‹œ ì‚¬ìš©)
            text_embeddings = []
            for idx, row in search_df.iterrows():
                text = f"{row['input']} {row['output']}"
                cache_key = f"{row['id']}_{row['data_type']}"
                
                if cache_key in self.embeddings_cache:
                    embedding = self.embeddings_cache[cache_key]
                else:
                    embedding = self.encoder.encode([text])[0]
                    self.embeddings_cache[cache_key] = embedding
                
                text_embeddings.append(embedding)
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            text_embeddings = np.array(text_embeddings)
            similarities = cosine_similarity(query_embedding, text_embeddings)[0]
            
            # ìƒìœ„ ê²°ê³¼ ì„ íƒ
            top_indices = similarities.argsort()[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                row = search_df.iloc[idx]
                result = {
                    'case_id': row['id'],
                    'case_number': row['case_number'],
                    'case_name': row['case_name'],
                    'court_code': row['court_code'],
                    'final_date': row['final_date'],
                    'case_type': row['data_type'],
                    'query': row['input'],
                    'answer': row['output'],
                    'instruction': row['instruction'],
                    'similarity_score': float(similarities[idx]),
                    'source': 'HuggingFace Dataset',
                    'rank': len(results) + 1
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def get_legal_interpretation(self, query: str) -> Dict:
        """ë²•ë¥  í•´ì„ ê²€ìƒ‰"""
        # í•´ì„ë¡€ ë°ì´í„°ë§Œ ê²€ìƒ‰
        results = self.search_similar_cases(query, top_k=1, case_type="í•´ì„ë¡€")
        
        if results:
            result = results[0]
            return {
                'question': result['query'],
                'answer': result['answer'],
                'context': f"ì‚¬ê±´ëª…: {result['case_name']}\në²•ì›: {result['court_code']}\në‚ ì§œ: {result['final_date']}",
                'similarity_score': result['similarity_score'],
                'source': result['source']
            }
        else:
            return {}
    
    def get_enhanced_case_analysis(self, case_description: str) -> Dict:
        """ì¢…í•© ì‚¬ê±´ ë¶„ì„"""
        try:
            # 1. ì‚¬ê±´ ë¶„ë¥˜ (ë°ì´í„° íƒ€ì…ë³„ ìœ ì‚¬ë„ í™•ì¸)
            case_classification = self._classify_case(case_description)
            
            # 2. ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰
            similar_precedents = self.search_similar_cases(case_description, top_k=5)
            
            # 3. ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰
            applicable_laws = self.search_similar_cases(case_description, top_k=3, case_type="ë²•ë ¹")
            
            # 4. ë²•ë¥  í•´ì„
            legal_interpretations = self.search_similar_cases(case_description, top_k=2, case_type="í•´ì„ë¡€")
            
            # 5. ë°ì´í„° ì†ŒìŠ¤ ì •ë³´
            data_sources = ["í—ˆê¹…í˜ì´ìŠ¤ í•œêµ­ì–´ ë²•ë¥  ë°ì´í„°ì…‹", "í˜•ì‚¬ë²• LLM ì‚¬ì „í•™ìŠµ ë°ì´í„°"]
            
            return {
                'case_classification': case_classification,
                'similar_precedents': similar_precedents,
                'applicable_laws': applicable_laws,
                'legal_interpretations': legal_interpretations,
                'data_sources': data_sources,
                'recommendations': [
                    "ìœ ì‚¬ íŒë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ ë³€í˜¸ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.",
                    "ê´€ë ¨ ë²•ë ¹ì„ ìì„¸íˆ ê²€í† í•˜ì„¸ìš”.",
                    "ì „ë¬¸ê°€ì™€ ìƒë‹´ì„ ë°›ìœ¼ì‹œê¸¸ ê¶Œí•©ë‹ˆë‹¤."
                ]
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def _classify_case(self, case_description: str) -> str:
        """ì‚¬ê±´ ë¶„ë¥˜"""
        try:
            # ê° ë°ì´í„° íƒ€ì…ë³„ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
            data_types = ['ê²°ì •ë¡€_QA', 'ë²•ë ¹_QA', 'íŒê²°ë¬¸_QA', 'í•´ì„ë¡€_QA']
            type_scores = {}
            
            for data_type in data_types:
                results = self.search_similar_cases(case_description, top_k=1, case_type=data_type.split('_')[0])
                if results:
                    type_scores[data_type] = results[0]['similarity_score']
                else:
                    type_scores[data_type] = 0
            
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ íƒ€ì… ë°˜í™˜
            if type_scores:
                best_type = max(type_scores, key=type_scores.get)
                return best_type.replace('_QA', '').replace('_SUM', '')
            else:
                return "ë¶„ë¥˜ ë¶ˆê°€"
                
        except Exception as e:
            return f"ë¶„ë¥˜ ì˜¤ë¥˜: {e}"
    
    def get_dataset_info(self) -> Dict:
        """ë°ì´í„°ì…‹ ì •ë³´ ë°˜í™˜"""
        if self.df.empty:
            return {}
        
        info = {
            'total_count': len(self.df),
            'data_types': self.df['data_type'].value_counts().to_dict(),
            'date_range': {
                'earliest': self.df['final_date'].min(),
                'latest': self.df['final_date'].max()
            },
            'source': self.dataset_name
        }
        
        return info 