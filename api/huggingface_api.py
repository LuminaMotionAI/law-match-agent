"""
í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” API í´ë˜ìŠ¤
"""

import os
from typing import List, Dict, Optional
from datasets import load_dataset
import pandas as pd
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import json

class HuggingFaceAPI:
    """í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” ë²•ë¥  ê²€ìƒ‰ API"""
    
    def __init__(self, dataset_name=None):
        """
        Args:
            dataset_name: í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ ì´ë¦„
        """
        # Streamlit secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
        self.dataset_name = dataset_name or self._get_dataset_name()
        self.hf_token = self._get_hf_token()
        self.dataset = None
        self.encoder = None
        self.embeddings_cache = {}
        
        # ë°ì´í„°í”„ë ˆì„ë“¤ (ë³µìˆ˜ ì†ŒìŠ¤)
        self.df = pd.DataFrame()
        self.curated_df = pd.DataFrame()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_encoder()
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        self._load_dataset()
        self._load_curated_dataset()
    
    def _get_dataset_name(self):
        """ë°ì´í„°ì…‹ ì´ë¦„ì„ Streamlit secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            import streamlit as st
            if hasattr(st, 'secrets') and 'huggingface' in st.secrets:
                return st.secrets['huggingface']['dataset_name']
        except:
            pass
        
        return os.getenv('HUGGINGFACE_DATASET_NAME', 'LuminaMotionAI/korean-legal-dataset')
    
    def _get_hf_token(self):
        """í—ˆê¹…í˜ì´ìŠ¤ í† í°ì„ Streamlit secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            import streamlit as st
            if hasattr(st, 'secrets') and 'huggingface' in st.secrets:
                return st.secrets['huggingface']['api_token']
        except:
            pass
        
        return os.getenv('HUGGINGFACE_API_TOKEN', None)

    def _initialize_encoder(self):
        """í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.encoder = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI')
            print("âœ… í•œêµ­ì–´ SBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.encoder = None
    
    def _load_dataset(self):
        """í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ"""
        try:
            print(f"ğŸ“¥ í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ë°ì´í„°ì…‹ ë¡œë”©: {self.dataset_name}")
            self.dataset = load_dataset(self.dataset_name)
            print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.dataset['all'])}ê°œ ë°ì´í„°")
            
            # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ (ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
            self.df = self.dataset['all'].to_pandas()
            print(f"âœ… ë°ì´í„°í”„ë ˆì„ ë³€í™˜ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ë¡œì»¬ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
            self._load_local_dataset()
    
    def _load_local_dataset(self):
        """ë¡œì»¬ ë°ì´í„°ì…‹ ë¡œë“œ (ë°±ì—…)"""
        try:
            from datasets import load_from_disk
            self.dataset = load_from_disk("korean_legal_dataset")
            self.df = self.dataset['all'].to_pandas()
            print(f"âœ… ë¡œì»¬ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ ë°ì´í„°")
        except Exception as e:
            print(f"âŒ ë¡œì»¬ ë°ì´í„°ì…‹ ë¡œë“œë„ ì‹¤íŒ¨: {e}")
            self.dataset = None
            self.df = pd.DataFrame()
    
    def _load_curated_dataset(self):
        """íë ˆì´í‹°ë“œ ë²•ë¥  ë°ì´í„°ì…‹ ë¡œë“œ"""
        try:
            curated_file = "curated_legal_dataset.json"
            if not os.path.exists(curated_file):
                print("âš ï¸ íë ˆì´í‹°ë“œ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            print("ğŸ“¥ íë ˆì´í‹°ë“œ ë²•ë¥  ë°ì´í„°ì…‹ ë¡œë”©...")
            
            with open(curated_file, 'r', encoding='utf-8') as f:
                curated_data = json.load(f)
            
            # íë ˆì´í‹°ë“œ ë°ì´í„°ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            curated_records = []
            
            for case_id, case_data in curated_data.get('precedents', {}).items():
                record = {
                    'id': case_id,
                    'case_number': case_id,
                    'case_name': case_data.get('title', ''),
                    'court_code': case_data.get('court', ''),
                    'final_date': case_data.get('date', ''),
                    'input': f"{case_data.get('title', '')} {case_data.get('summary', '')}",
                    'output': f"íŒê²° ìš”ì§€: {case_data.get('summary', '')}\n"
                             f"í•µì‹¬ ë²•ë¦¬: {'; '.join(case_data.get('key_legal_points', []))}\n"
                             f"ì ìš© ë²•ë ¹: {'; '.join(case_data.get('applicable_laws', []))}\n"
                             f"í˜•ëŸ‰: {case_data.get('sentence', '')}\n"
                             f"ì†í•´ë°°ìƒ: {case_data.get('compensation', '')}",
                    'data_type': f"íë ˆì´í‹°ë“œ_{case_data.get('category', 'ì¼ë°˜')}",
                    'law_class': case_data.get('category', ''),
                    'importance': case_data.get('importance', 'ë³´í†µ'),
                    'applicable_laws': case_data.get('applicable_laws', []),
                    'key_legal_points': case_data.get('key_legal_points', [])
                }
                curated_records.append(record)
            
            self.curated_df = pd.DataFrame(curated_records)
            print(f"âœ… íë ˆì´í‹°ë“œ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.curated_df)}ê°œ ê³ í’ˆì§ˆ íŒë¡€")
            
        except Exception as e:
            print(f"âŒ íë ˆì´í‹°ë“œ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.curated_df = pd.DataFrame()
    
    def search_similar_cases(self, query: str, top_k: int = 5, case_type: str = None) -> List[Dict]:
        """ìœ ì‚¬í•œ ì‚¬ë¡€ ê²€ìƒ‰ (íë ˆì´í‹°ë“œ + í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„° í†µí•©)"""
        all_results = []
        
        # 1. íë ˆì´í‹°ë“œ ê³ í’ˆì§ˆ ë°ì´í„°ì—ì„œ ë¨¼ì € ê²€ìƒ‰
        if not self.curated_df.empty:
            curated_results = self._search_in_dataframe(
                self.curated_df, query, top_k//2 + 1, case_type, source="íë ˆì´í‹°ë“œ"
            )
            all_results.extend(curated_results)
        
        # 2. í—ˆê¹…í˜ì´ìŠ¤ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ê²€ìƒ‰
        if not self.df.empty:
            hf_results = self._search_in_dataframe(
                self.df, query, top_k, case_type, source="í—ˆê¹…í˜ì´ìŠ¤"
            )
            all_results.extend(hf_results)
        
        # 3. ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        seen_cases = set()
        final_results = []
        
        # íë ˆì´í‹°ë“œ ê²°ê³¼ë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ì²˜ë¦¬
        for result in all_results:
            case_key = f"{result['case_number']}_{result['case_name']}"
            if case_key not in seen_cases:
                seen_cases.add(case_key)
                final_results.append(result)
                
                if len(final_results) >= top_k:
                    break
        
        # ìˆœìœ„ ì¬ì¡°ì •
        for i, result in enumerate(final_results):
            result['rank'] = i + 1
        
        return final_results
    
    def _search_in_dataframe(self, df: pd.DataFrame, query: str, top_k: int, case_type: str = None, source: str = "Unknown") -> List[Dict]:
        """íŠ¹ì • ë°ì´í„°í”„ë ˆì„ì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰"""
        if df.empty or self.encoder is None:
            return []
        
        try:
            # ë°ì´í„° í•„í„°ë§
            search_df = df.copy()
            if case_type and case_type != "ì „ì²´":
                case_type_map = {
                    "í•´ì„ë¡€": "í•´ì„ë¡€_QA",
                    "íŒê²°ë¬¸": "íŒê²°ë¬¸_QA", 
                    "ê²°ì •ë¡€": "ê²°ì •ë¡€_QA",
                    "ë²•ë ¹": "ë²•ë ¹_QA"
                }
                if case_type in case_type_map:
                    search_df = search_df[search_df['data_type'].str.contains(case_type_map[case_type])]
            
            if search_df.empty:
                return []
            
            # 1ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ (ë¹ ë¥¸ ê²€ìƒ‰)
            query_lower = query.lower()
            keyword_matches = []
            
            for idx, row in search_df.iterrows():
                input_text = str(row['input']).lower()
                output_text = str(row['output']).lower()
                case_name = str(row['case_name']).lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                keyword_score = 0
                if query_lower in input_text or query_lower in output_text or query_lower in case_name:
                    keyword_score += 3
                
                # ë¶€ë¶„ í‚¤ì›Œë“œ ë§¤ì¹­
                query_words = query_lower.split()
                for word in query_words:
                    if len(word) > 1:  # í•œ ê¸€ìëŠ” ì œì™¸
                        if word in input_text or word in output_text:
                            keyword_score += 1
                
                # íŠ¹ë³„ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ (ë²•ë¥  ìš©ì–´)
                legal_keywords = ['ì¡°', 'í•­', 'í˜¸', 'ë²•', 'ì£„', 'í˜•ë²•', 'ë¯¼ë²•', 'ìƒë²•', 'ìŠ¤í† í‚¹', 'ì„±ë²”ì£„', 'ì‚¬ê¸°']
                for keyword in legal_keywords:
                    if keyword in query_lower and keyword in input_text or keyword in output_text:
                        keyword_score += 2
                
                if keyword_score > 0:
                    keyword_matches.append((idx, keyword_score))
            
            # 2ë‹¨ê³„: í‚¤ì›Œë“œ ë§¤ì¹­ëœ ê²ƒì´ ìˆìœ¼ë©´ ìš°ì„  ì²˜ë¦¬
            if keyword_matches:
                # í‚¤ì›Œë“œ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
                keyword_matches.sort(key=lambda x: x[1], reverse=True)
                selected_indices = [idx for idx, score in keyword_matches[:top_k*3]]  # ë” ë§ì´ ì„ íƒ
                filtered_df = search_df.loc[selected_indices]
            else:
                # í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš© (íë ˆì´í‹°ë“œëŠ” ì‘ìœ¼ë‹ˆê¹Œ ì „ì²´, í—ˆê¹…í˜ì´ìŠ¤ëŠ” ìƒ˜í”Œë§)
                if source == "íë ˆì´í‹°ë“œ":
                    filtered_df = search_df
                else:
                    filtered_df = search_df.sample(min(1000, len(search_df)))  # ì„±ëŠ¥ì„ ìœ„í•´ ìƒ˜í”Œë§
            
            # 3ë‹¨ê³„: ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
            query_embedding = self.encoder.encode([query])
            
            text_embeddings = []
            valid_indices = []
            
            for idx, row in filtered_df.iterrows():
                text = f"{row['input']} {row['output']}"
                cache_key = f"{row['id']}_{row['data_type']}_{source}"
                
                if cache_key in self.embeddings_cache:
                    embedding = self.embeddings_cache[cache_key]
                else:
                    embedding = self.encoder.encode([text])[0]
                    self.embeddings_cache[cache_key] = embedding
                
                text_embeddings.append(embedding)
                valid_indices.append(idx)
            
            if not text_embeddings:
                return []
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            text_embeddings = np.array(text_embeddings)
            similarities = cosine_similarity(query_embedding, text_embeddings)[0]
            
            # ìƒìœ„ ê²°ê³¼ ì„ íƒ
            top_indices = similarities.argsort()[-top_k:][::-1]
            
            results = []
            for i, embedding_idx in enumerate(top_indices):
                original_idx = valid_indices[embedding_idx]
                row = search_df.loc[original_idx]
                similarity_score = similarities[embedding_idx]
                
                # ìµœì†Œ ìœ ì‚¬ë„ í•„í„°ë§ (íë ˆì´í‹°ë“œëŠ” ë” ê´€ëŒ€í•˜ê²Œ)
                min_similarity = 0.05 if source == "íë ˆì´í‹°ë“œ" else 0.1
                if similarity_score < min_similarity:
                    continue
                
                result = {
                    'case_id': row['id'],
                    'case_number': row['case_number'],
                    'case_name': row['case_name'],
                    'court_code': row['court_code'],
                    'final_date': row['final_date'],
                    'input': row['input'],
                    'output': row['output'],
                    'data_type': row['data_type'],
                    'similarity': similarity_score,
                    'source': source,
                    'rank': i + 1
                }
                
                # íë ˆì´í‹°ë“œ ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´ í¬í•¨
                if source == "íë ˆì´í‹°ë“œ" and 'applicable_laws' in row:
                    result['applicable_laws'] = row.get('applicable_laws', [])
                    result['key_legal_points'] = row.get('key_legal_points', [])
                    result['importance'] = row.get('importance', 'ë³´í†µ')
                
                results.append(result)
            
            return results
            
        except Exception as e:
            print(f"âŒ {source} ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_legal_interpretation(self, query: str) -> Dict:
        """ë²•ë¥  í•´ì„ ê²€ìƒ‰"""
        # í•´ì„ë¡€ ë°ì´í„°ë§Œ ê²€ìƒ‰
        results = self.search_similar_cases(query, top_k=1, case_type="í•´ì„ë¡€")
        
        if results:
            result = results[0]
            return {
                'question': result['query'],
                'answer': result['answer'],
                'context': f"ì‚¬ê±´ëª…: {result['case_name']}\në²•ì›: {result['court_code']}\në‚ ì§œ: {result['final_date']}",
                'similarity_score': result['similarity_score'],
                'source': result['source']
            }
        else:
            return {}
    
    def get_enhanced_case_analysis(self, case_description: str) -> Dict:
        """ì¢…í•© ì‚¬ê±´ ë¶„ì„"""
        try:
            # 1. ì‚¬ê±´ ë¶„ë¥˜ (ë°ì´í„° íƒ€ì…ë³„ ìœ ì‚¬ë„ í™•ì¸)
            case_classification = self._classify_case(case_description)
            
            # 2. ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰
            similar_precedents = self.search_similar_cases(case_description, top_k=5)
            
            # 3. ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰
            applicable_laws = self.search_similar_cases(case_description, top_k=3, case_type="ë²•ë ¹")
            
            # 4. ë²•ë¥  í•´ì„
            legal_interpretations = self.search_similar_cases(case_description, top_k=2, case_type="í•´ì„ë¡€")
            
            # 5. ë°ì´í„° ì†ŒìŠ¤ ì •ë³´
            data_sources = ["í—ˆê¹…í˜ì´ìŠ¤ í•œêµ­ì–´ ë²•ë¥  ë°ì´í„°ì…‹", "í˜•ì‚¬ë²• LLM ì‚¬ì „í•™ìŠµ ë°ì´í„°"]
            
            return {
                'case_classification': case_classification,
                'similar_precedents': similar_precedents,
                'applicable_laws': applicable_laws,
                'legal_interpretations': legal_interpretations,
                'data_sources': data_sources,
                'recommendations': [
                    "ìœ ì‚¬ íŒë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ ë³€í˜¸ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.",
                    "ê´€ë ¨ ë²•ë ¹ì„ ìì„¸íˆ ê²€í† í•˜ì„¸ìš”.",
                    "ì „ë¬¸ê°€ì™€ ìƒë‹´ì„ ë°›ìœ¼ì‹œê¸¸ ê¶Œí•©ë‹ˆë‹¤."
                ]
            }
            
        except Exception as e:
            return {'error': str(e)}
    
    def _classify_case(self, case_description: str) -> str:
        """ì‚¬ê±´ ë¶„ë¥˜"""
        try:
            # ê° ë°ì´í„° íƒ€ì…ë³„ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
            data_types = ['ê²°ì •ë¡€_QA', 'ë²•ë ¹_QA', 'íŒê²°ë¬¸_QA', 'í•´ì„ë¡€_QA']
            type_scores = {}
            
            for data_type in data_types:
                results = self.search_similar_cases(case_description, top_k=1, case_type=data_type.split('_')[0])
                if results:
                    type_scores[data_type] = results[0]['similarity_score']
                else:
                    type_scores[data_type] = 0
            
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ íƒ€ì… ë°˜í™˜
            if type_scores:
                best_type = max(type_scores, key=type_scores.get)
                return best_type.replace('_QA', '').replace('_SUM', '')
            else:
                return "ë¶„ë¥˜ ë¶ˆê°€"
                
        except Exception as e:
            return f"ë¶„ë¥˜ ì˜¤ë¥˜: {e}"
    
    def get_dataset_info(self) -> Dict:
        """ë°ì´í„°ì…‹ ì •ë³´ ë°˜í™˜ (í—ˆê¹…í˜ì´ìŠ¤ + íë ˆì´í‹°ë“œ í†µí•©)"""
        total_hf = len(self.df) if not self.df.empty else 0
        total_curated = len(self.curated_df) if not self.curated_df.empty else 0
        total_count = total_hf + total_curated
        
        if total_count == 0:
            return {}
        
        # ë°ì´í„° íƒ€ì… í†µê³„
        data_types = {}
        
        # í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„° íƒ€ì…
        if not self.df.empty:
            hf_types = self.df['data_type'].value_counts().to_dict()
            for dtype, count in hf_types.items():
                data_types[f"HF_{dtype}"] = count
        
        # íë ˆì´í‹°ë“œ ë°ì´í„° íƒ€ì…
        if not self.curated_df.empty:
            curated_types = self.curated_df['data_type'].value_counts().to_dict()
            for dtype, count in curated_types.items():
                data_types[dtype] = count
        
        # ë‚ ì§œ ë²”ìœ„
        all_dates = []
        if not self.df.empty:
            all_dates.extend(self.df['final_date'].dropna().tolist())
        if not self.curated_df.empty:
            all_dates.extend(self.curated_df['final_date'].dropna().tolist())
        
        date_range = {'earliest': 'Unknown', 'latest': 'Unknown'}
        if all_dates:
            all_dates = [d for d in all_dates if d and d != '']
            if all_dates:
                date_range = {
                    'earliest': min(all_dates),
                    'latest': max(all_dates)
                }
        
        info = {
            'total_count': total_count,
            'huggingface_count': total_hf,
            'curated_count': total_curated,
            'data_types': data_types,
            'date_range': date_range,
            'source': f"{self.dataset_name} + íë ˆì´í‹°ë“œ ë°ì´í„°",
            'data_quality': f"ëŒ€ìš©ëŸ‰ ë°ì´í„° {total_hf:,}ê°œ + ê³ í’ˆì§ˆ ë°ì´í„° {total_curated}ê°œ"
        }
        
        return info 