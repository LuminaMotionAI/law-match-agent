import re
import numpy as np
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

class TextAnalyzer:
    """í…ìŠ¤íŠ¸ ë¶„ì„ ë° ìœ ì‚¬ë„ ê³„ì‚° í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        í…ìŠ¤íŠ¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  sentence transformer ëª¨ë¸ëª…
        """
        self.model = None
        self._try_load_model()
        
        self.index = None
        self.precedent_embeddings = None
        self.precedent_data = None
    
    def _try_load_model(self):
        """SentenceTransformer ëª¨ë¸ ë¡œë”© ì‹œë„"""
        try:
            # transformers ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ í™˜ê²½ ì„¤ì •
            import os
            os.environ["TOKENIZERS_PARALLELISM"] = "false"
            
            from sentence_transformers import SentenceTransformer
            import torch
            
            # ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ ëª¨ë¸ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹œë„
            model_options = [
                "all-MiniLM-L6-v2",
                "paraphrase-MiniLM-L6-v2", 
                "distiluse-base-multilingual-cased"
            ]
            
            for model_name in model_options:
                try:
                    print(f"ğŸ“š {model_name} ëª¨ë¸ ë¡œë”©ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                    self.model = SentenceTransformer(model_name, device='cpu')
                    print(f"âœ… {model_name} ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    break
                except Exception as e:
                    print(f"âŒ {model_name} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
                    
            if self.model is None:
                print("âš ï¸ ëª¨ë“  SentenceTransformer ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
                
        except ImportError:
            print("âš ï¸ sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ SentenceTransformer ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            print("ğŸ’¡ ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
    
    def preprocess_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        try:
            # ì¤„ë°”ê¿ˆ ë¬¸ì ì •ë¦¬
            text = text.replace('\n', ' ').replace('\r', ' ')
            
            # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
            text = re.sub(r'\s+', ' ', text)
            
            # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬ (ë²•ë¥  ë¬¸ì„œì—ì„œ í•„ìš”í•œ ê²ƒë“¤ì€ ìœ ì§€)
            text = re.sub(r'[^\w\sê°€-í£.,()[\]{}:;!?-]', ' ', text)
            
            # ì•ë’¤ ê³µë°± ì œê±°
            text = text.strip()
            
            return text
            
        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return text
    
    def extract_legal_terms(self, text: str) -> List[str]:
        """
        ë²•ë¥  ìš©ì–´ ì¶”ì¶œ
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì¶”ì¶œëœ ë²•ë¥  ìš©ì–´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë²•ë¥  ìš©ì–´ íŒ¨í„´ë“¤
            patterns = [
                r'(í˜•ë²•|ë¯¼ë²•|ìƒë²•|í–‰ì •ë²•|í˜•ì‚¬ì†Œì†¡ë²•|ë¯¼ì‚¬ì†Œì†¡ë²•)\s*ì œ?\s*(\d+ì¡°)',  # ë²•ë ¹ ì¡°ë¬¸
                r'(ì§•ì—­|ë²Œê¸ˆ|ê³¼ë£Œ|êµ¬ë¥˜|ê³¼íƒœë£Œ|ì§‘í–‰ìœ ì˜ˆ|ì„ ê³ ìœ ì˜ˆ)\s*(\d+ë…„?\s*\d*ê°œì›”?|\d+ë§Œì›?)',  # í˜•ëŸ‰
                r'(ê³ ë°œ|ê³ ì†Œ|ê¸°ì†Œ|ê³ ë°œ|ì²´í¬|êµ¬ì†|ì˜ì¥|ìˆ˜ì‚¬|ì¬íŒ|íŒê²°|ì„ ê³ )',  # ë²•ì  ì ˆì°¨
                r'(ì‚¬ê¸°|ì ˆë„|ê°•ë„|ì‚´ì¸|í­í–‰|ìƒí•´|í˜‘ë°•|ê³µê°ˆ|ëª¨ìš•|ëª…ì˜ˆí›¼ì†|ë„ë°•|ìŒì£¼ìš´ì „)',  # ë²”ì£„ ìœ í˜•
                r'(ìœ„ìë£Œ|ì†í•´ë°°ìƒ|ì •ì‹ ì í”¼í•´|ì¬ì‚°í”¼í•´|í”¼í•´ë³´ìƒ)',  # í”¼í•´ ê´€ë ¨
                r'(ì¦ê±°|ì¦ì¸|ì§„ìˆ |ìë°±|ë¬µë¹„ê¶Œ|ë³€í˜¸ì‚¬|ê²€ì‚¬|íŒì‚¬)',  # ë²•ì • ê´€ë ¨
            ]
            
            legal_terms = []
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        legal_terms.extend([term for term in match if term])
                    else:
                        legal_terms.append(match)
            
            # ì¤‘ë³µ ì œê±°
            legal_terms = list(set(legal_terms))
            
            return legal_terms
            
        except Exception as e:
            print(f"ë²•ë¥  ìš©ì–´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """
        ë‘ í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ (0-1)
        """
        try:
            if self.model is None:
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„)
                return self._calculate_basic_similarity(text1, text2)
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text1 = self.preprocess_text(text1)
            text2 = self.preprocess_text(text2)
            
            # ì„ë² ë”© ìƒì„±
            embeddings = self.model.encode([text1, text2])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (sklearn ì—†ì´)
            emb1, emb2 = embeddings[0], embeddings[1]
            dot_product = np.dot(emb1, emb2)
            norm_a = np.linalg.norm(emb1)
            norm_b = np.linalg.norm(emb2)
            similarity = dot_product / (norm_a * norm_b)
            
            return float(similarity)
            
        except Exception as e:
            print(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return self._calculate_basic_similarity(text1, text2)
    
    def build_precedent_index(self, precedents: List[Dict]):
        """
        íŒë¡€ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ êµ¬ì¶•
        
        Args:
            precedents: íŒë¡€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            if self.model is None:
                # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì €ì¥
                self.precedent_data = precedents
                print("ğŸ“š í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒë¡€ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
                return
            
            # íŒë¡€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
            texts = []
            for precedent in precedents:
                text = f"{precedent.get('title', '')} {precedent.get('summary', '')} {precedent.get('keywords', '')}"
                texts.append(self.preprocess_text(text))
            
            # ì„ë² ë”© ìƒì„±
            embeddings = self.model.encode(texts)
            
            # ì •ê·œí™”
            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
            
            # ì°¸ì¡° ë°ì´í„° ì €ì¥ (FAISS ì—†ì´ numpy ë°°ì—´ ì‚¬ìš©)
            self.precedent_embeddings = embeddings
            self.precedent_data = precedents
            
            print(f"âœ… {len(precedents)}ê°œì˜ íŒë¡€ ì„ë² ë”© ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ íŒë¡€ ì¸ë±ìŠ¤ êµ¬ì¶• ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì €ì¥
            self.precedent_data = precedents
            print("ğŸ’¡ í…ìŠ¤íŠ¸ ê¸°ë°˜ ëª¨ë“œë¡œ ì „í™˜ë©ë‹ˆë‹¤.")
    
    def search_similar_precedents(self, query: str, top_k: int = 5) -> List[Tuple[Dict, float]]:
        """
        ìœ ì‚¬í•œ íŒë¡€ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            
        Returns:
            (íŒë¡€, ìœ ì‚¬ë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if self.precedent_data is None:
                print("âš ï¸ íŒë¡€ ë°ì´í„°ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            if self.model is None or self.precedent_embeddings is None:
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰
                return self._search_precedents_basic(query, top_k)
            
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬ ë° ì„ë² ë”©
            query = self.preprocess_text(query)
            query_embedding = self.model.encode([query])
            
            # ì •ê·œí™”
            query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ëª¨ë“  íŒë¡€ì™€)
            similarities = np.dot(self.precedent_embeddings, query_embedding.T).flatten()
            
            # ìƒìœ„ kê°œ ê²°ê³¼ ì„ íƒ
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            # ê²°ê³¼ êµ¬ì„±
            results = []
            for idx in top_indices:
                if idx < len(self.precedent_data):
                    score = similarities[idx]
                    results.append((self.precedent_data[idx], float(score)))
            
            return results
            
        except Exception as e:
            print(f"íŒë¡€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return self._search_precedents_basic(query, top_k)
    
    def extract_key_phrases(self, text: str, max_phrases: int = 10) -> List[str]:
        """
        í•µì‹¬ êµ¬ë¬¸ ì¶”ì¶œ
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            max_phrases: ìµœëŒ€ ì¶”ì¶œí•  êµ¬ë¬¸ ìˆ˜
            
        Returns:
            í•µì‹¬ êµ¬ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë¬¸ì¥ ë¶„ë¦¬
            sentences = re.split(r'[.!?]', text)
            
            # ì§§ì€ ë¬¸ì¥ë“¤ ì œê±°
            sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
            
            if not sentences:
                return []
            
            if self.model is None:
                # ê¸°ë³¸ ë°©ì‹: ë²•ë¥  ìš©ì–´ê°€ ë§ì€ ë¬¸ì¥ì„ ìš°ì„ 
                return self._extract_key_phrases_basic(sentences, max_phrases)
            
            # ë¬¸ì¥ ì„ë² ë”© ìƒì„±
            embeddings = self.model.encode(sentences)
            
            # ë¬¸ì„œ ì „ì²´ ì„ë² ë”© ìƒì„±
            doc_embedding = self.model.encode([text])
            
            # ê° ë¬¸ì¥ê³¼ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
            doc_embedding_norm = doc_embedding / np.linalg.norm(doc_embedding, axis=1, keepdims=True)
            embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
            similarities = np.dot(embeddings_norm, doc_embedding_norm.T).flatten()
            
            # ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            top_indices = np.argsort(similarities)[::-1][:max_phrases]
            
            # ìƒìœ„ ë¬¸ì¥ë“¤ ë°˜í™˜
            key_phrases = [sentences[i] for i in top_indices]
            
            return key_phrases
            
        except Exception as e:
            print(f"í•µì‹¬ êµ¬ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return self._extract_key_phrases_basic(sentences, max_phrases) if sentences else []
    
    def analyze_text_structure(self, text: str) -> Dict:
        """
        í…ìŠ¤íŠ¸ êµ¬ì¡° ë¶„ì„
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            êµ¬ì¡° ë¶„ì„ ê²°ê³¼
        """
        try:
            # ê¸°ë³¸ í†µê³„
            word_count = len(text.split())
            char_count = len(text)
            sentence_count = len(re.split(r'[.!?]', text))
            
            # ë²•ë¥  ìš©ì–´ ë¶„ì„
            legal_terms = self.extract_legal_terms(text)
            
            # ìˆ«ì íŒ¨í„´ ë¶„ì„
            numbers = re.findall(r'\d+', text)
            dates = re.findall(r'\d{4}[ë…„.-]\d{1,2}[ì›”.-]\d{1,2}[ì¼]?', text)
            
            # ë¬¸ì¥ ê¸¸ì´ ë¶„ì„
            sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]
            avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0
            
            return {
                'word_count': word_count,
                'char_count': char_count,
                'sentence_count': sentence_count,
                'legal_terms': legal_terms,
                'legal_term_count': len(legal_terms),
                'numbers': numbers,
                'dates': dates,
                'avg_sentence_length': avg_sentence_length,
                'complexity_score': self._calculate_complexity(text)
            }
            
        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ êµ¬ì¡° ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}
    
    def _calculate_complexity(self, text: str) -> float:
        """
        í…ìŠ¤íŠ¸ ë³µì¡ë„ ê³„ì‚°
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            ë³µì¡ë„ ì ìˆ˜ (0-1)
        """
        try:
            # ë¬¸ì¥ ê¸¸ì´ ë¶„ì„
            sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]
            if not sentences:
                return 0.0
            
            avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
            
            # ì–´íœ˜ ë‹¤ì–‘ì„± ë¶„ì„
            words = text.split()
            unique_words = set(words)
            lexical_diversity = len(unique_words) / len(words) if words else 0
            
            # ë²•ë¥  ìš©ì–´ ë°€ë„
            legal_terms = self.extract_legal_terms(text)
            legal_term_density = len(legal_terms) / len(words) if words else 0
            
            # ë³µì¡ë„ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            complexity = (
                (avg_sentence_length / 50) * 0.4 +  # ë¬¸ì¥ ê¸¸ì´ (ìµœëŒ€ 50ë‹¨ì–´ ê°€ì •)
                lexical_diversity * 0.3 +  # ì–´íœ˜ ë‹¤ì–‘ì„±
                legal_term_density * 0.3  # ë²•ë¥  ìš©ì–´ ë°€ë„
            )
            
            return min(complexity, 1.0)
            
        except Exception as e:
            print(f"ë³µì¡ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def find_similar_patterns(self, texts: List[str], threshold: float = 0.7) -> List[List[int]]:
        """
        ìœ ì‚¬í•œ íŒ¨í„´ì„ ê°€ì§„ í…ìŠ¤íŠ¸ ê·¸ë£¹ ì°¾ê¸°
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ë“¤ì˜ ì¸ë±ìŠ¤ ê·¸ë£¹
        """
        try:
            if len(texts) < 2:
                return []
            
            if self.model is None:
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê·¸ë£¹í•‘
                return self._find_similar_patterns_basic(texts, threshold)
            
            # ì„ë² ë”© ìƒì„±
            embeddings = self.model.encode(texts)
            
            # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
            similarity_matrix = cosine_similarity(embeddings)
            
            # ìœ ì‚¬í•œ ê·¸ë£¹ ì°¾ê¸°
            groups = []
            visited = set()
            
            for i in range(len(texts)):
                if i in visited:
                    continue
                
                group = [i]
                visited.add(i)
                
                for j in range(i + 1, len(texts)):
                    if j not in visited and similarity_matrix[i][j] >= threshold:
                        group.append(j)
                        visited.add(j)
                
                if len(group) > 1:
                    groups.append(group)
            
            return groups
            
        except Exception as e:
            print(f"ìœ ì‚¬ íŒ¨í„´ ì°¾ê¸° ì˜¤ë¥˜: {e}")
            return self._find_similar_patterns_basic(texts, threshold)
    
    def _calculate_basic_similarity(self, text1: str, text2: str) -> float:
        """
        ê¸°ë³¸ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (ëª¨ë¸ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
        
        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ (0-1)
        """
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text1 = self.preprocess_text(text1.lower())
            text2 = self.preprocess_text(text2.lower())
            
            # ë‹¨ì–´ ì§‘í•© ìƒì„±
            words1 = set(text1.split())
            words2 = set(text2.split())
            
            # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            if union == 0:
                return 0.0
            
            jaccard_similarity = intersection / union
            
            # ë²•ë¥  ìš©ì–´ ê°€ì¤‘ì¹˜ ì ìš©
            legal_terms1 = set(self.extract_legal_terms(text1))
            legal_terms2 = set(self.extract_legal_terms(text2))
            legal_intersection = len(legal_terms1.intersection(legal_terms2))
            
            if legal_intersection > 0:
                # ë²•ë¥  ìš©ì–´ê°€ ì¼ì¹˜í•˜ë©´ ê°€ì¤‘ì¹˜ ì¶”ê°€
                jaccard_similarity += 0.2 * (legal_intersection / max(len(legal_terms1), len(legal_terms2), 1))
            
            return min(jaccard_similarity, 1.0)
            
        except Exception as e:
            print(f"ê¸°ë³¸ ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _search_precedents_basic(self, query: str, top_k: int = 5) -> List[Tuple[Dict, float]]:
        """
        ê¸°ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒë¡€ ê²€ìƒ‰ (ëª¨ë¸ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            
        Returns:
            (íŒë¡€, ìœ ì‚¬ë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not self.precedent_data:
                return []
            
            query = self.preprocess_text(query.lower())
            query_words = set(query.split())
            query_legal_terms = set(self.extract_legal_terms(query))
            
            results = []
            
            for precedent in self.precedent_data:
                # íŒë¡€ í…ìŠ¤íŠ¸ ìƒì„±
                prec_text = f"{precedent.get('title', '')} {precedent.get('summary', '')} {precedent.get('keywords', '')}"
                prec_text = self.preprocess_text(prec_text.lower())
                prec_words = set(prec_text.split())
                prec_legal_terms = set(self.extract_legal_terms(prec_text))
                
                # ê¸°ë³¸ ë‹¨ì–´ ë§¤ì¹­ ì ìˆ˜
                word_intersection = len(query_words.intersection(prec_words))
                word_union = len(query_words.union(prec_words))
                word_score = word_intersection / word_union if word_union > 0 else 0
                
                # ë²•ë¥  ìš©ì–´ ë§¤ì¹­ ì ìˆ˜ (ê°€ì¤‘ì¹˜ ì ìš©)
                legal_intersection = len(query_legal_terms.intersection(prec_legal_terms))
                legal_score = legal_intersection / max(len(query_legal_terms), 1) if legal_intersection > 0 else 0
                
                # ì œëª© ë§¤ì¹­ ë³´ë„ˆìŠ¤
                title_score = 0
                if precedent.get('title'):
                    title_words = set(precedent['title'].lower().split())
                    title_intersection = len(query_words.intersection(title_words))
                    title_score = title_intersection / len(title_words) if title_words else 0
                
                # ì¢…í•© ì ìˆ˜ ê³„ì‚°
                total_score = (word_score * 0.4) + (legal_score * 0.4) + (title_score * 0.2)
                
                if total_score > 0:
                    results.append((precedent, total_score))
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ kê°œ ë°˜í™˜
            results.sort(key=lambda x: x[1], reverse=True)
            return results[:top_k]
            
        except Exception as e:
            print(f"ê¸°ë³¸ íŒë¡€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_key_phrases_basic(self, sentences: List[str], max_phrases: int = 10) -> List[str]:
        """
        ê¸°ë³¸ í•µì‹¬ êµ¬ë¬¸ ì¶”ì¶œ (ëª¨ë¸ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
        
        Args:
            sentences: ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            max_phrases: ìµœëŒ€ ì¶”ì¶œí•  êµ¬ë¬¸ ìˆ˜
            
        Returns:
            í•µì‹¬ êµ¬ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        try:
            scored_sentences = []
            
            for sentence in sentences:
                score = 0
                
                # ë²•ë¥  ìš©ì–´ ì ìˆ˜
                legal_terms = self.extract_legal_terms(sentence)
                score += len(legal_terms) * 2
                
                # ë¬¸ì¥ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ ë¬¸ì¥ ì„ í˜¸)
                length = len(sentence.split())
                if 5 <= length <= 30:
                    score += 1
                
                # ìˆ«ìë‚˜ ë‚ ì§œ í¬í•¨ ì ìˆ˜ (ë²•ë¥  ë¬¸ì„œì—ì„œ ì¤‘ìš”í•  ìˆ˜ ìˆìŒ)
                if re.search(r'\d+', sentence):
                    score += 1
                
                # íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì ìˆ˜
                important_keywords = ['íŒê²°', 'ì„ ê³ ', 'í˜•ëŸ‰', 'ë²Œê¸ˆ', 'ì§•ì—­', 'í”¼ê³ ì¸', 'í”¼í•´ì', 'ë²•ì›', 'ì¡°í•­']
                for keyword in important_keywords:
                    if keyword in sentence:
                        score += 1
                
                scored_sentences.append((sentence, score))
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            scored_sentences.sort(key=lambda x: x[1], reverse=True)
            
            # ìƒìœ„ ë¬¸ì¥ë“¤ ë°˜í™˜
            return [sentence for sentence, _ in scored_sentences[:max_phrases]]
            
        except Exception as e:
            print(f"ê¸°ë³¸ í•µì‹¬ êµ¬ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return sentences[:max_phrases] if sentences else []
    
    def _find_similar_patterns_basic(self, texts: List[str], threshold: float = 0.7) -> List[List[int]]:
        """
        ê¸°ë³¸ í…ìŠ¤íŠ¸ ìœ ì‚¬ íŒ¨í„´ ì°¾ê¸° (ëª¨ë¸ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ë“¤ì˜ ì¸ë±ìŠ¤ ê·¸ë£¹
        """
        try:
            if len(texts) < 2:
                return []
            
            groups = []
            visited = set()
            
            for i in range(len(texts)):
                if i in visited:
                    continue
                
                group = [i]
                visited.add(i)
                
                for j in range(i + 1, len(texts)):
                    if j not in visited:
                        similarity = self._calculate_basic_similarity(texts[i], texts[j])
                        if similarity >= threshold:
                            group.append(j)
                            visited.add(j)
                
                if len(group) > 1:
                    groups.append(group)
            
            return groups
            
        except Exception as e:
            print(f"ê¸°ë³¸ ìœ ì‚¬ íŒ¨í„´ ì°¾ê¸° ì˜¤ë¥˜: {e}")
            return [] 